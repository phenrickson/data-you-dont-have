---
# title: "Measurement, Causality, and <br> The Data You Don't Have </br>"
title: "The Data You Don't Have"
subtitle: "Where is the value in data science?"
author: "Phil Henrickson, PhD <br> Data Scientist <br> AE Business Solutions"
format:
    clean-revealjs
css: styles.css
editor: source
execute:
  cache: true
  freeze: false
---


```{r}
#| include: false
#| label: packages
#| cache: false
library(targets)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(tidymodels)
library(quarto)
library(gtExtras)
library(patchwork)
library(ggridges)
library(ggh4x)
library(gt)
library(gtExtras)
library(cfbplotR)

# src code
tar_source("R")
source("R/framingham.R")
source("R/reports.R")

theme_set(
  theme_light()+
    theme(
      legend.position = 'top',
      panel.grid.minor = element_blank()
    )
)

```

```{r}
#| include: false
#| label: cfb
team_info <- tar_read("cfbd_team_info_tbl")
team_estimates <- tar_read("season_team_estimates")
team_scores <- tar_read("team_scores")
draws <- tar_read("games_draws")
sims <- tar_read("games_sims")

# # team info
# team_info <- cfbfastR::cfbd_team_info(year = 2024)

# estimates
team_scores = tar_read("team_scores")
team_category_estimates = tar_read("season_team_category_estimates")

draws = tar_read("games_draws")
sims = tar_read("games_sims")
games = tar_read("season_game_info")

# team info
team_info = cfbfastR::cfbd_team_info(only_fbs = T) |> as_tibble()

# espn fpi
espn_fpi =
  map(c(2011:2024),
      ~ cfbfastR::espn_ratings_fpi(year = .x)
  ) |>
  list_rbind()

# get betting lines
betting_lines =
  load_betting_lines(year = 2024)

# game info
game_info =
  load_games(year = 2024)

# predictions for games
game_predictions <-
  sims |>
  join_team_divisions(games = game_info) |>
  prepare_fcs_teams() |>
  left_join(
    game_info |>
      add_game_outcomes() |>
      select(game_id, start_date, week, home_points, away_points, home_margin, home_win, total_points, completed)
  ) |>
  arrange(start_date)

# add betting lines
game_predictions_and_betting_lines = 
  game_predictions |>
  join_betting_lines(betting = betting_lines |>
                       mutate(spread = case_when(!is.na(spread_open) ~ spread_open,
                                                 TRUE ~ spread))
  )

# join sims with betting lines
sims_and_betting_lines =
  sims |>
  join_betting_lines(betting = betting_lines)
```

```{r}
#| include: false
#| label: playoff sims

tar_load(playoff_teams)

# postseason
postseason_predictions = 
  game_predictions |>
  filter(season_type == 'postseason')

# sims
tar_load(playoff_sims)
tar_load(quarterfinal_sims)
tar_load(semifinal_sims)
tar_load(championship_sims)

# estimates frozen from before each week
tar_load(pre_playoff_estimates)
tar_load(pre_quarterfinal_estimates)
tar_load(pre_semifinal_estimates)
tar_load(pre_championship_estimates)

# table of playoff team ratings
pre_playoff_ratings = 
  pre_playoff_estimates |>
  select(season, team, score = overall, offense, defense, special)

pre_quarterfinal_ratings = 
  pre_quarterfinal_estimates |>
  select(season, team, score = overall, offense, defense, special)

pre_semifinal_ratings = 
  pre_semifinal_estimates |>
  select(season, team, score = overall, offense, defense, special)

pre_championship_ratings = 
  pre_championship_estimates |>
  select(season, team, score = overall, offense, defense, special)

```



```{r}
#| include: false
#| label: framingham

source("R/framingham.R")

plot_ridge_chd = function(data, ...) {
  
  data |>
    plot_ridge() +
    labs(y = "density") +
    scale_color_framingham()
}

# custom color scale
scale_color_framingham <- function(values = c("deepskyblue1", "red")) {
  list(
    scale_fill_manual(values = values),
    scale_color_manual(values = values)
  )
}

# load in data
chd_data = 
  load_data() |>
  prepare_data() |>
  filter(prev_chd == 0) |>
  add_ten_year_status(var = "chd")

# sample data for table
set.seed(9)
chd_samp = 
  chd_data |>
  mutate(id = as.character(randid)) |>
  select(id, sex, totchol, age, sysbp, diabp, smoker = cursmoke, bmi, heartrte, glucose, ten_year_chd) |>
  sample_n(50)

# fit models by sex and period
chd_results = 
  chd_data |>
  nest(data = -c(period, sex)) |>
  mutate(fit = map(data, fit_model)) |>
  mutate(tidied =  map(fit, tidy, robust = T, conf.int = T))

```


# Background

---

"For a decade now, [the role of] Data Scientist has been in the spotlight. AI experts had salaries that rivaled those of sports superstars.

. . .

In the search for fame and fortune, hundreds of young professionals entered into what seemed [a frenetic golden rush]{.fg style="--col: #C6B90B"}... Whole new industries sprang around the hype.

. . .

Consulting specialists promised millions if your company could [unlock the potential of data]{.fg style="--col: #C6B90B"}.

. . .

AI, or Machine Learning, has been called the [new electricity]{.fg style="--col: #C6B90B"} and data, the [new oil]{.fg style="--col: #C6B90B"}."[^1]

[^1]: Source: [Matheus Facure, *Causal Inference for the Brave and True*](Alveshttps://matheusfacure.github.io/python-causality-handbook/01-Introduction-To-Causality.html)

---

<!-- This trend has only accelerated in recent years with innovations in the form of large language models and generative artificial intelligence. -->

<!-- . . . -->

But there's a problem.

. . .

Is all of this investment in data science actually worth anything?

##  {background-image="images/cio_ai_roi.png" background-position="center" background-color="white" background-size="contain"}

##  {background-image="images/gs_gen_ai.png" background-position="center" background-color="white" background-size="contain"}

##  {background-image="images/ai_bubble.png" background-position="center" background-color="white" background-size="contain"}

---

Most organizations want to advance their capabilities in DS/ML/AI.

. . .

At the same time, most organizations struggle to find value from them.

. . .

In my humble estimation, I think this is because they are looking for value in the wrong places.

---

"During all of this time...

. . .

-   economists were [trying to answer]{.fg style="--col: #75BAFF"} what is the true impact of education on one’s earnings

. . .

-   biostatisticians were [trying to understand]{.fg style="--col: #75BAFF"} if saturated fat led to a higher chance of a heart attack

. . .

-   psychologists were [trying to understand]{.fg style="--col: #75BAFF"} if words of affirmation led indeed to a happier marriage.

. . .

We forgot about those who have been doing “old-fashioned” science with data all along."[^2]

[^2]: Source: [Matheus Facure, *Causal Inference for the Brave and True*](Alveshttps://matheusfacure.github.io/python-causality-handbook/01-Introduction-To-Causality.html)

---

What, really, is the value of data science?

. . .

The value of data science is simply that of science - it is the process by which we [try to understand]{.fg style="--col: #75BAFF"} the world around us.

. . .

It allows us to discover [cause and effect]{.fg style="--col: #75BAFF"}; it allows us to [measure things we care about]{.fg style="--col: #75BAFF"}. It helps us understand the data we have and the data that we don’t.

---

To illustrate, I want to tell you **two stories** of "old fashioned" data science in action.

. . .

These stories involve two very different, yet related topics.

. . .

The second story is about an important topic that affects us all, something that weighs on us everyday and affects the physical well-being of ourselves and our loved ones:  <span class="fragment">**college football**.</span>

. . .

The first story is about **heart disease**.

# 1) What causes cardiovascular failure?

---

Have you ever gone to the doctor and received your ten-year cardiovascular risk score?

. . .

Have you ever wondered where that score comes from?

. . .

Would you have guessed that it had something to do with Franklin Delano Roosevelt?

##  {background-image="images/fdr_before.jpg" background-position="center" background-color="white" background-size="contain"}

---

President Roosevelt died on April 12, 1945, at the age of 63, from cerebral hemorrhage with a blood pressure of **300/190 mmHg**.

. . .

![](images/blood_pressure.png){fig-align="center"}

---

By the 1940s, cardiovascular disease had become the number one cause of mortality among Americans, accounting for 1 in 2 deaths.

At this time, **almost nothing was known about the causes of heart failure**.

. . .

Prevention and treatment were so poorly understood that most Americans accepted **early death from heart disease as unavoidable**.

---

For example:

. . .

In 1932, candidate Roosevelt's campaign office released medical records showing his blood pressure to be **140/100 mmHg**, which did not prompt any medical intervention.

. . .

By 1941, the President experienced a gradual rise in blood pressure to **188/105 mmHg**.

. . .

In March 1944, Dr. Bruenn noted that the patient appeared “slightly cyanotic” with blood pressure of **186/108 mmHg**.

. . .

A month after coming under Dr. Bruenn's care, Roosevelt's blood pressure had risen to **240/130 mmHg**.

---

<!-- In February, 1945, Lord Charles Moran, Churchill's personal physician wrote: "the Americans here cannot bring themselves to believe that he is finished. His daughter thinks he is not really ill, and his doctor backs her up. **I give him only a few months to live**." -->

<!-- . . . -->

<!-- FDR died two months later. -->

<!-- --- -->

FDR's death in April of 1945 prompted a national call for the study of cardiovascular disease.

## {background-image="images/heart_act.png" background-position="center" background-color="white" background-size="contain"}

---

FDR's death in April of 1945 prompted a national call for the study of cardiovascular disease.

On June 16, 1948, President Harry Truman signed into law the National Heart Act. This law approved a **twenty-year epidemiological heart study** and established the National Heart Institute.

. . .

This study was the brainchild of **Joseph Mountin**, a physician from Hartford, Wisconsin.

---

![](images/mountin.jpg){fig-align="center"}

---

How do you determine the causes of long term heart risk?

. . .


**Joseph Mountin** recognized that the problem demanded a long term study; **collecting the necessary data**.

. . .

> "Observations of population characteristics must be made **well before disease becomes overt** if the relationship of these characteristics to the development of the disease is to be established with reasonable certainty."

---

How do you determine the causes of long term heart risk?

. . .

- Study a large group of people over a long period of time who have not yet developed overt symptoms of cardiovascular disease.

. . .

- Collect data on every individual at the start of the study and during regularly scheduled follow-ups.

. . .

- Observe them. Eventually, some of the individuals will experience cardiovascular disease.

. . .

- Examine the relationship between data collected at the beginning of the study and the onset of the disease.


---

So that is what they decided to do.

. . .

The town of **Framingham, Massachusetts** was chosen as the location for the study.

---

![](images/framingham_town.jpg){fig-align="center"}

<!-- ## -->
<!-- ::: {layout-ncol=2} -->

<!-- ![](images/framingham_aerial.jpg){width=90%} -->

<!-- ![](images/framingham_town.jpg){width=60%} -->
<!-- ::: -->

<!-- . . . -->

---

So that is what they decided to do.

The town of **Framingham, Massachusetts** was chosen as the location for the study.

The one-time farming community was now a factory town of 28,000 middle-class residents of predominantly European origin... and was **"therefore considered to be representative of the United States in the 1940s"**.

::: aside

The original cohort was recruited between 1948 and 1952 and consisted of 5209 residents aged 28 to 62 years. Women comprised more than half of the participants in the study. The study's inclusion of women contrasted with contemporaneous epidemiological studies, which had very small numbers of women or excluded them altogether.

:::

---

What data did they choose to collect?

. . .

A committee of specialists had to **speculate about the potential causes** and develop a variety of hypotheses to guide their data collection.

. . .

They cast a pretty wide net in collecting data on individuals.

## {background-image="images/data_collection.png" background-position="center" background-color="white" background-size="contain"}

---

![](images/framingham_carbon_paper.png){fig-align="center"}

---

I requested the (anonymized) data from the Framingham study for the purpose of this talk.

. . .

This is a portion of the clinical data they collected.

---

```{r}
#| warning: false
#| message: false
#| class: scroll
gt_framingham =
  chd_samp |>
  gt() |>
  gt::cols_align(align = c("center")) |>
  gt::fmt_missing() |>
  gt::cols_hide("id") |>
  gtExtras::gt_theme_espn() |>
  gtExtras::gt_theme_nytimes()

gt_framingham

```

---

The goal of the study was to identify how individual **health features** relate to an **outcome**, future onset of heart disease.

---

```{r}
#| warning: false
#| message: false
#| class: scroll

gt_framingham

```

---

```{r}
#| warning: false
#| message: false
#| class: scroll
gt_framingham_features =
  gt_framingham |>
  gt::data_color(columns = -c("ten_year_chd", "id"), direction = "column", palette = "Greys", na_color = "white")

gt_framingham_features

```

---

```{r}
#| warning: false
#| message: false
#| class: scroll
gt_framingham_features |>
  gt::data_color(columns = c("ten_year_chd"), palette = c("deepskyblue", "red"))

```

---

What is the relationship between *age*, *blood pressure*, *resting heart rate* and the onset of future heart disease?

. . .

Just by looking at the data, you can start to spot features that are predictive of future heart disease. 

---

Notably, and as we would probably expect, *age*. 

```{r}
#| message: false
chd_data |>
  filter(period == 1) |>
  select(ten_year_chd, sex, age) |>
  plot_ridge_chd()+
  theme(strip.text.x = element_blank())+
  xlab("age")

```

---

And the effect of *age* is slightly different for men and women, with men showing higher risk of heart disease at younger ages.

```{r}
#| message: false
chd_data |>
  filter(period == 1) |>
  select(ten_year_chd, sex, age) |>
  plot_ridge_chd() +
  scale_color_framingham() +
  facet_grid2(sex ~ ., scales = "free_y")+
  xlab("age")

```

---

But the most striking observations comes from looking at *systolic blood pressure*.

```{r}
#| message: false
chd_data |>
  filter(period == 1) |>
  select(ten_year_chd, sex, sysbp) |>
  plot_ridge_chd() +
  scale_color_framingham() +
  facet_grid2(sex ~ ., scales = "free_y")+
  xlab("systolic blood pressure")

```

---

What is the relationship between *age*, *blood pressure*, *resting heart rate* and the onset of future heart disease?

This is the purpose of a **model**, which we fit to the data to **estimate the effect** of these features on our outcome.

---

```{r}

chd_results |>
  filter(period == 3) |>
  unnest(tidied) |>
  plot_coef(color_var = 'sex', size = 0.25)+
  theme(legend.title = element_blank())
```

And we end up finding that *age*, *systolic blood pressure*, *hdlc*, and *smoking* impact an individual's future risk of heart disease.

---

How does blood pressure affect the risk of heart disease?

```{r}
#| message: false
#| warning: false

chd_results |>
  filter(period == 1) |>
  plot_effects(var = 'sysbp') +
  facet_wrap(sex  ~., scales = "free_x")

```

---

This is what the researchers found in the original Framingham study.

. . .

The first major findings from the original cohort were published in 1957, almost a decade after the initial participant was examined.

. . .

They found a nearly 4 fold increase in coronary heart disease incidence per 1000 persons among hypertensive participants (≥160/95 mmHg).

---

If you were wondering what this model would have predicted for FDR based on his blood pressure.

```{r}
#| message: false
#| warning: false
fdr_prob =
  chd_results |>
  filter(period == 1, sex == 'male') |>
  plot_effects(var = 'sysbp') +
  facet_wrap(sex  ~., scales = "free_x")

fdr_prob +
  geom_point(data = tibble(sysbp = 300, .pred = .95))+
  annotate(x = 280, y = .7, geom = "text", label = "FDR's risk in 1945")+
  geom_segment(aes(x = 290, y = .81, xend = 299, yend =.93),
               arrow = arrow(length = unit(0.25, "cm")))

```

---

Much of what we know about the causes of heart disease (exercise, diet, smoking) was found in the years to come in the more than 3000 papers published using data from the Framingham Heart Study.

. . .

These findings form the basis of the **Framingham Risk Score**, a simple model for estimating long term risk of cardiovascular disease that is used to this day.

---

##  {background-image="images/framingham_risk_score.png" background-position="center" background-color="white" background-size="contain"}

---

Much of what we know about the causes of heart disease (exercise, diet, smoking) was found in the years to come in the more than 3000 papers published using data from the Framingham Heart Study.

These findings form the basis of the **Framingham Risk Score**, a simple model for estimating long term risk of cardiovascular disease that is used to this day.

The Framingham study continues; it is now on its **third generation of residents**, examining the effects of family history and genetics.

---

Why am I telling you about this?

. . .

1) The data **you choose to collect, and not collect**, is part of the scientific process.

. . .

2) All of the technology in the world does not matter if you do not **understand your problem** and the **data and methodology that would help you solve it**.

---

The methods we use to understand something **as simple as heart disease** are the same ones we use to make predictions for something far more serious:  <span class="fragment">**college football**.</span>


# 2) How do you predict football games?

---

![](images/playoff_nc_preview.png){fig-align="center"}

---

Have you ever looked at pre-game betting lines and win probabilities for football games?

. . .

Have you ever wondered how they make these predictions?

. . .

Would you have guessed that it had something to do with a quarterback who played in the 1970s?

##  {background-image="images/virgil_carter.jpg" background-position="center" background-color="white" background-size="contain"}

---

**Virgil Carter** was an NFL quarterback who played for the Bears and the Bengals in the 1970s.

. . .

**While also being a quarterback in the NFL**, Virgil Carter earned a Master's degree from Northwestern and taught statistics and mathematics at Xavier University.

. . .

The focus of his research, naturally, was football.

##  {background-image="images/virgil_carter_solved.png" background-position="center" background-color="white" background-size="contain"}

##  {background-image="images/operations_research_football.png" background-position="center" background-color="white" background-size="contain"}

---

How do you evaluate a football team?

. . .

Virgil Carter's idea was to **measure the value of individual plays** in terms of **expected points**.

. . .

To illustrate: how many points is each of the following plays worth?

---

![](gifs/eMsJcuIooJg_00-01-35_00-01-48.gif){fig-align="center"}

---

![](gifs/eMsJcuIooJg_00-01-28_00-01-35.gif){fig-align="center"}

---

![](gifs/G9w9Lu43UhU_00-01-23_00-01-32.gif){fig-align="center"}

---

To answer questions like these, Virgil Carter manually collected play-by-play data from the entire 1969 NFL season.

. . .

He wanted to calculate the **expected value** of field position in terms of **points**, or **expected points**.

. . .

![](images/1971_expected_points.png){fig-align="center"}

---

What did he mean by **expected points**?

. . .

Suppose your offense starts a drive on its own 15 yard line. On average, how many points do teams in this situation typically go on to score?

. . .

From his calculations: **-0.637**. Why?

. . .

On average, teams starting from their 15 yard line are *less likely* to score next than their opponents.

---

But, imagine your offense completes a pass for 10 yards and moves to your 25. Now, your expected points is positive: **0.236**. Why?

. . .

![](images/1971_expected_points.png){fig-align="center"}

. . .

On average, teams on their own 25 yard line were *more likely* to score next than their opponents.

---

That 10 yard pass improved your situation quite a bit, changing your expected points from **-0.637** to **0.236**. 

. . .

The play's **expected points added** is **0.873**.

. . .

This is how we measure the value of plays in college football.

---

![](gifs/eMsJcuIooJg_00-01-35_00-01-48.gif){fig-align="center"}

::: {style="font-size: 75%;text-align: center;"}
Expected Points Before Play: 1.80\
Expected Points After Play: 7.00\
**Expected Points Added: 5.20**
:::

---

![](gifs/eMsJcuIooJg_00-01-28_00-01-35.gif){fig-align="center"}

::: {style="font-size: 75%;text-align: center;"}
Expected Points Before Play: 0.67\
Expected Points After Play: 1.80\
**Expected Points Added: 1.13**
:::

---

![](gifs/G9w9Lu43UhU_00-01-23_00-01-32.gif){fig-align="center"}

::: {style="font-size: 75%;text-align: center;"}
Expected Points Before Play: 0.20\
Expected Points After Play: -3.25\
**Expected Points Added: -3.45**
:::

---

Note: these estimates are not based on Virgil Carter's 1969 NFL season, but my own expected points model trained on all college football plays from 2007-2019.

---

![](images/predicted_probability_points.png){fig-align="center"}

---

![](images/my_expected_points.png){fig-align="center"}

---

Virgil Carter's idea forms the basis of modern football analytics and how we evaluate teams and players:

Good offenses generate points _in expectation_.\
Good defenses prevent points _in expectation_.

. . .

We evaluate teams based on plays and their efficiency in terms of **expected points**.

. . .

This turns out to be an effective way to **measure a team's performance** for the purpose of predicting games.

---

How do you predict college football games?

. . .

You **measure** the efficiency of **every team's offense and defense** based on every play that has occurred over the course of a season via expected points.[^3]

. . .

You simultaneously estimate a rating for every team's offense and defense, conditional on every other team's offense and defense.[^4]

[^3]: Technically, this is fairly involved and this measurement also includes plays from previous seasons with increasingly less weight assigned to them. A full discussion of this is done would take some time; I would be happy to talk about it in **exhausting detail** with you later.

[^4]: This is also fairly involved but can be accomplished via a ridge regression; again, I would be happy to talk about it in **exhausting detail** with you later.

---

For example, this is where teams were rated in terms of efficiency on the eve of the College Football Playoff Semi Finals.

---

```{r}
#| class: scroll-long
team_scores_by_week = function(data, season, week, team_info) {
  
  data |>
    inner_join(
      tibble(season = season)
    ) |>
    add_season_week() |>
    group_by(team) |>
    arrange(season, week) |>
    group_by(team, season, week) |>
    slice_head(n = 1) |>
    ungroup() |>
    group_by(team) |>
    mutate(diff = round(score - dplyr::lag(score, 1), 2)) |>
    ungroup() |>
    inner_join(
      tibble(week = week)
    ) |>
    arrange(desc(season_week), desc(score)) |>
    mutate(rank = rank(-score)) |>
    select(season, week, rank, team, everything()) |>
    join_team_info(teams = team_info)
  
}

team_scores |>
  team_scores_by_week(
    season = 2024, 
    week = 20, 
    team_info = team_info
  ) |>
  mutate(logo = team) |>
  select(season, season_type, season_week, week, rank, logo, team, score, diff, offense, defense, special) |>
  gt_tbl() |>
  gt::cols_hide(
    columns = c(season_type, season_week, week, special)
  ) |>
  gt::fmt_number(
    columns = c(score),
    decimals = 2
  ) |>
  gt::fmt_number(
    columns = c(offense, defense, special),
    decimals = 3
  ) |>
  gt::cols_align(
    columns = -c(team),
    align = "center"
  ) |>
  gt::cols_label(
    season = "Season",
    week = "Week",
    rank = "Rank",
    logo = "Logo",
    team = "Team",
    score = "Team Score",
    diff = "∆ Score",
    offense = "Offense",
    defense = "Defense",
    special = "Special Teams"
  ) |>
  # gt::cols_width(
  #   season ~ px(75),
  #   week ~ px(75),
  #   rank ~ px(75),
  #   logo ~ px(75)
  # ) |>
  cfbplotR::gt_fmt_cfb_logo(columns = "logo", height = 20) |>
  gt::opt_interactive(
    use_compact_mode = F,
    use_filters = T,
    page_size_default = 10
  ) |>
  gt_est_color(
    columns = c("offense", "defense"),
    domain = c(-.55, .55)
  ) |>
  # gt::cols_merge(
  #   columns = c("score", "diff"),
  #   pattern = "{1} ({2})"
  # ) |>
  gt_est_color(
    columns = "score",
    domain = c(-40, 40)
  ) |>
  gt::data_color(
    columns = c("diff"),
    domain = c(-12, 12),
    palette = rev(my_gt_palette()),
    na_color = 'white'
  ) |>
  add_gt_formatting()

```

---

You then use these **estimates of team efficiency** to simulate the outcome of games.

. . .

For example, these are the simulations for the semi final games.

---

```{r}
#| message: false
#| warning: false
#| fig-height: 6.5
#| fig-align: center
game_info |>
  filter(grepl("Semifinal", notes)) |>
  head(1) |>
  inner_join(
    semifinal_sims
  ) |>
  add_game_outcomes() |>
  plot_playoff_sims(bins = 70)

```

---

```{r}
#| message: false
#| warning: false
#| fig-height: 6.5
#| fig-align: center
game_info |>
  filter(grepl("Semifinal", notes)) |>
  tail(1) |>
  inner_join(
    semifinal_sims
  ) |>
  add_game_outcomes() |>
  plot_playoff_sims(bins = 65)

```

---

Is that really how they do it?

. . .

I was working on my own series of models for evaluating teams and eventually decided to compare my work to that of ESPN and Vegas.

##  {background-image="images/espn_fpi.png" background-position="center" background-color="white" background-size="contain"}

##  {background-image="images/espn_fpi_pre_nc.png" background-position="center" background-color="white" background-size="contain"}

---

```{r}
#| message: false
#| warning: false
#| fig-height: 6
#| fig-align: center

plot_vs_fpi = function(data) {
  
  b =
    data |>
    ggplot(aes(x=score, y=fpi))+
    facet_wrap(season ~., ncol = 4)+
    theme_cfb()+
    geom_abline(slope = 1,
                linetype = 'dashed')+
    #   tune::coord_obs_pred()+
    xlab("Phil's Team Rating")+
    ylab("ESPN Team Rating")+
    coord_cartesian(xlim = c(-45, 45),
                    ylim = c(-45, 45))
  
  b
}

vs_fpi =
  team_scores |>
  find_team_season_score()  |>
  select(season, team, score) |>
  inner_join(
    team_info |>
      adjust_team_names(cols = "school") |>
      select(team = school, team_id)
  ) |>
  inner_join(
    espn_fpi |>
      select(season = year, team_id, team_abbreviation, fpi) |>
      mutate(fpi = as.numeric(fpi))
  )

p =
  vs_fpi |>
  plot_vs_fpi()

p +
  geom_point(alpha = 0.25)+
  ggpubr::stat_cor(method = 'spearman', aes(label = ..r.label..))


```

::: {style="font-size: 75%;"}

My end of year team ratings compared to the ESPN college football power index.

:::

---

```{r}
#| class: scroll-long
vs_fpi |>
  filter(season == 2024) |>
  pivot_longer(
    cols = c("score", "fpi")
  ) |>
  group_by(name) |>
  mutate(rank = rank(-value)) |>
  pivot_wider(
    names_from = c("name"),
    values_from = c("value", "rank")
  ) |>
  arrange(desc(value_score)) |>
  mutate(logo = team) |>
  select(season, logo, team, value_score, rank_score, value_fpi, rank_fpi) |>
  head(25) |>
  gt_tbl() |>
  gt::fmt_number(
    columns = c(starts_with("value_")),
    decimals = 2
  ) |>
  gt::fmt_number(
    columns = c(starts_with("rank_")),
    decimals = 0
  ) |>
  gt::cols_align(
    align = "center"
  ) |>
  gt::cols_label(
    value_score = "Phil Rating",
    value_fpi = "ESPN FPI",
    rank_score = "Phil Rank",
    rank_fpi = "ESPN Rank"
  ) |>
  # gt::cols_merge(
  #   columns = c("value_score", "rank_score"),
  #   pattern = "{1} ({2})"
  # ) |>
  # gt::cols_merge(
  #   columns = c("value_fpi", "rank_fpi"),
  #   pattern = "{1} ({2})"
  # ) |>
  # ) |>
  gt_est_color(
    columns = starts_with("value_"),
    domain = c(-40, 40)
  ) |>
  gt::opt_interactive(page_size_default = 15) |>
  cfbplotR::gt_fmt_cfb_logo(columns = "logo", height = 20) |>
  add_gt_formatting()

# gtExtras::gt_theme_nytimes()

```

---

::: {style="font-size: 75%;"}

My spread vs opening spreads from providers throughout the 2024 season.

:::

```{r}
#| message: false
#| warning: false
#| fig-align: center
sims_and_betting_lines |>
  filter(!is.na(provider)) |>
  ggplot(aes(x=pred_margin, y= -1*spread_open))+
  geom_point(alpha = 0.5)+
  theme_cfb()+
  tune::coord_obs_pred()+
  geom_abline(slope = 1, linetype = 'dashed')+
  ggpubr::stat_cor(aes(label = ..r.label..)) +
  facet_wrap(paste(season) ~.)+
  xlab("Phil's Estimated Spread")+
  ylab("ESPN Bet Spread")+
  coord_cartesian(xlim = c(-30, 60),
                  ylim = c(-30, 60))+
  facet_wrap(provider ~., ncol = 2)

```

::: {style="font-size: 75%;"}

Betting lines for individual games are a little bit harder to crack; they are surely accounting for things that I (currently) am not, such as rest/injuries/travel.

:::

---

However, my model performs about as well in predicting games.

```{r}

sims_and_betting_predictions = 
  sims_and_betting_lines |>
  select(season, season_type, season_week, game_id, 
         home_team, away_team, 
         pred_margin, spread_margin, 
         provider
  ) |>
  inner_join(
    game_info
  ) |>
  pivot_wider(
    names_from = "provider",
    values_from = "spread_margin"
  ) |>
  select(season, season_type, season_week, game_id, 
         home_team, away_team, home_points, away_points,
         Phil = pred_margin,
         Bovada,
         `ESPN Bet`,
         DraftKings
  ) |>
  add_game_outcomes() |>
  pivot_longer(
    cols = c("Phil", "Bovada", "ESPN Bet", "DraftKings"),
    names_to = c("provider"),
    values_to = c("pred_margin")
  ) |>
  mutate(
    home_pred = case_when(
      pred_margin > 0 ~ 'yes',
      pred_margin < 0 ~ 'no'
    ),
    home_pred = factor(home_pred, levels = c("yes", "no"))
  )

sims_and_betting_predictions |>
  add_correct() |>
  filter(!is.na(correct)) |>
  group_by(season, provider, correct) |>
  count() |>
  pivot_wider(names_from = c("correct"),
              values_from = c("n")) |>
  mutate(record = paste(yes, no, sep = "-")) |>
  mutate(games = yes + no,
         accuracy = yes / games) |>
  select(
    season, model = provider, games, record, accuracy
  ) |>
  ungroup() |>
  arrange(desc(accuracy)) |>
  gt_tbl() |>
  gt::fmt_number(columns = c("accuracy"),
                 decimals = 3) |>
  gt::cols_align(
    columns = c("games", "record", "accuracy"),
    align = "center"
  ) |>
  gtExtras::gt_theme_nytimes()

```

. . .

My record against the spread for each provider.

```{r}

game_predictions_and_betting_lines |>
  add_game_outcomes() |>
  filter(completed == T) |>
  filter(!is.na(spread)) |>
  arrange(week) |>
  select(game_id, season, week, home_team, away_team, home_prob, home_margin, pred_margin, provider, spread, spread_open, spread_margin) |>
  add_spread_cover() |>
  assess_spread(groups = c("season", "provider")) |>
  mutate(week = 'overall') |>
  select(season, provider, games, record, accuracy = .estimate) |>
  gt_tbl() |>
  gt::fmt_number(columns = c("accuracy"), decimals = 3) |>
  gt::cols_align(
    align = "center"
  ) |>
  gtExtras::gt_theme_nytimes()

```

---

Beyond prediction, using a model allows us to measure and monitor a team's performance based on their underlying play, not just their record.

. . .

For instance, Wisconsin's estimated performance by week since 2019.

---

```{r}
#| warning: false
#| message: false
#| fig-height: 6.5
#| fig-align: center
team_scores |>
  filter(season >= 2019) |>
  plot_team_by_week(team = 'Wisconsin', ranking = c(10, 25))
```

---

```{r}
#| warning: false
#| message: false
#| fig-height: 6.5
#| fig-align: center
team_scores |>
  filter(season >= 2010) |>
  plot_team_by_week(team = 'Wisconsin', ranking = c(10, 25))

```

---

We can break this down further based on pass/rush, which paints a pretty grim picture for Wisconsin's offense.

---

```{r}
#| warning: false
#| message: false
#| fig-height: 6.5
#| fig-align: center

team_category_estimates |>
  filter(season >= 2010) |>
  plot_team_efficiency_by_category_and_week(team = 'Wisconsin', ranking = c(10, 25))


```

---

Why am I telling you about this?

. . .

1) Models are about more than just prediction; they enable us to **make sense of patterns in data** and **measure things we care about**.

. . .

2) The best work and the best predictions tend to come from **really trying to understand the thing you are predicting**.

# Bonus Round

Predicting the CFB Playoff

##

How do you predict the college football playoff?

You use your measures of team performance to simulate how the playoff might unfold. You do this thousands of times.

. . .

I ran my simulations on the eve of the playoff.

. . .

I will admit, I was suspicious of the results and my own model. But this was the result.

---

```{r}
#| message: false
#| warning: false
playoff_sims |>
  gt_playoff_probs(
    ratings = pre_playoff_ratings
  ) |>
  gt::tab_header("2024 College Football Playoff Simulations",
                 subtitle = paste("Results based on 10,000 simulations from team efficiency model"))

```

---

```{r}
#| fig-height: 6.5
playoff_sims |>
  summarize_wins_by_team() |>
  plot_bracket()

```

---

My model has been stubborn that Ohio State is the best team in college football since the end of September.

```{r}
#| fig-align: center

team_scores |>
  add_season_week() |>
  filter(week > 0) |>
  filter(season == 2024) |>
  team_rankings_tile()

```

<!-- --- -->

<!-- ```{r} -->
<!-- #| message: false -->
<!-- #| warning: false -->
<!-- team_scores |> -->
<!--   filter(season == 2024) |> -->
<!--   inner_join(team_info |> -->
<!--                select(team = school, abbreviation)) |> -->
<!--   add_season_week() |> -->
<!--   group_by(season, team) |> -->
<!--   mutate(start_label = case_when(week == min(week) ~ abbreviation), -->
<!--          end_label = case_when(week == max(week) ~ abbreviation)) |> -->
<!--   inner_join( -->
<!--     tibble(team = playoff_teams) -->
<!--   ) |> -->
<!--   # inner_join( -->
<!--   #   team_info |> -->
<!--   #     filter(conference == 'Big Ten') |> -->
<!--   #     select(team = school) -->
<!--   # ) |> -->
<!--   plot_team_lines(span = .13) + -->
<!--   label_team(var = end_label, size = 2.5, nudge_x = 1.1)+ -->
<!--   label_team(var = start_label, size = 2.5, nudge_x = -0.9) -->

<!-- ``` -->

<!-- --- -->

---

It has only grown more confident about Ohio State after each round.

---

```{r}
#| message: false
#| warning: false
#| fig-width: 10
#| column: page

playoff_sims |>
  gt_playoff_probs(
    ratings = pre_playoff_ratings
  ) |>
  gt::tab_header("2024 College Football Playoff Simulations",
                 subtitle = paste("Results based on 10,000 simulations from team efficiency model"))
```

---

```{r}
#| message: false
#| warning: false
#| fig-width: 10
#| column: page

quarterfinal_sims |>
  gt_quarterfinal_probs(
    ratings = pre_quarterfinal_ratings
  ) |>
  gt::tab_header("2024 College Football Playoff Simulations",
                 subtitle = paste("Results based on 10,000 simulations from team efficiency model"))

```

---

```{r}
#| message: false
#| warning: false
#| fig-width: 10
#| column: page

semifinal_sims |>
  gt_semifinal_probs(
    ratings = pre_semifinal_ratings
  ) |>
  gt::tab_header("2024 College Football Playoff Simulations",
                 subtitle = paste("Results based on 10,000 simulations from team efficiency model"))

```

---

So far, the playoff has gone almost exactly to script.

```{r}
#| message: false
#| warning: false
postseason_predictions |>
  inner_join(
    game_info |>
      filter(grepl("College Football Playoff", notes)) |>
      filter(!grepl("Championship", notes))
  ) |>
  add_season_week() |>
  select(
    season, start_date, week, home, away, pred_margin, home_pred, home_prob, home_margin, home_win
  ) |>
  add_correct() |>
  mutate(prediction = case_when(pred_margin > 0 ~ paste(home, pred_margin, sep = " by "),
                                pred_margin < 0 ~ paste(away, -pred_margin, sep = " by "))) |>
  mutate(actual = case_when(home_margin > 0 ~ paste(home, home_margin, sep = " by "),
                            home_margin < 0 ~ paste(away, -home_margin, sep = " by "))) |>
  select(season,  week, home, away, home_prob, prediction, actual, correct) |>
  gt_tbl() |>
  gt::cols_align(
    columns = c(season, week, home_prob, prediction, actual, correct),
    align = "center"
  ) |>
  gt::fmt_number(
    columns = c("home_prob"),
    decimals = 3
  ) |>
  gt::data_color(
    columns = c("home_prob"),
    domain = c(0, 1),
    palette = c("white", "dodgerblue2")
  ) |>
  gt::cols_label(
    home_prob = "Pr(Home Win)",
    correct = "Correct?"
  ) |>
  gtExtras::gt_theme_nytimes() |>
  gt::sub_values(
    values = "yes",
    replacement = "✓"
  )
# gt_correct_color()

```


---

Which brings us to the national championship, which for my money is between the two best teams in college football.

. . .

```{r}
#| message: false
#| warning: false

team_category_estimates |>
  prepare_team_category_estimates() |>
  unnest(everything()) |>
  add_season_week() |>
  group_by(season, week, team) |>
  slice_tail(n = 1) |>
  ungroup() |>
  filter(season == 2024, week == 21) |>
  inner_join(
    tibble(team = c("Notre Dame", "Ohio State"))
  ) |>
  arrange(desc(estimate_pass_offense)) |>
  gt_tbl() |>
  gt::fmt_number(
    contains("estimate"),
    decimals = 3
  ) |>
  gt::cols_merge(
    columns = contains("pass_offense"),
    pattern = "{1}<< ({2})>>"
  ) |>
  gt::cols_merge(
    columns = contains("rush_offense"),
    pattern = "{1}<< ({2})>>"
  ) |>
  gt::cols_merge(
    columns = contains("pass_defense"),
    pattern = "{1}<< ({2})>>"
  ) |>
  gt::cols_merge(
    columns = contains("rush_defense"),
    pattern = "{1}<< ({2})>>"
  ) |>
  gt::cols_label(
    season = "Season",
    week = "Week",
    team = "Team",
    estimate_pass_offense = "Pass Offense",
    estimate_rush_offense = "Run Offense",
    estimate_pass_defense = "Pass Defense",
    estimate_rush_defense = "Run Defense"
  )  |>
  gt::cols_hide(
    c(season_week, week)
  ) |>
  gt::cols_align(
    c(contains("offense"), contains("defense"), "season", "week"),
    align = "center"
  ) |>
  gt_est_color(columns = c(contains("estimate")),
               domain = c(-0.75, 0.75)) |>
  #cfbplotR::gt_fmt_cfb_logo(columns = "logo") |>
  gt::cols_label(
    logo = "Logo"
  ) |>
  gt::cols_hide("logo") |>
  gt::cols_align(
    align = "center",
    columns = "logo"
  ) |>
  add_gt_formatting() |>
  gtExtras::gt_theme_nytimes()

```

---

```{r}
#| fig-height: 6.5
#| fig-align: center
team_scores |>
  team_scores_by_week(season = 2024, week = 21, team_info = team_info) |>
  plot_team_scores()

```

---

```{r}
#| warning: false
#| message: false
#| fig-height: 6.5
#| fig-align: center
team_category_estimates |>
  filter(season >= 2019) |>
  plot_team_efficiency_by_category_and_week(team = 'Ohio State', ranking = c(10, 25))
```

---

```{r}
#| warning: false
#| message: false
#| fig-height: 6.5
#| fig-align: center
team_category_estimates |>
  filter(season >= 2019) |>
  plot_team_efficiency_by_category_and_week(team = 'Notre Dame', ranking = c(10, 25))
```

---

What's the prediction?

---

```{r}
#| message: false
#| warning: false
#| fig-width: 10
#| column: page

championship_sims |>
  gt_championship_probs(
    ratings = pre_championship_ratings
  ) |>
  gt::tab_header("2024 College Football Playoff Simulations",
                 subtitle = paste("Results based on 10,000 simulations from team efficiency model"))

```

---

~~Ireland wins but Krum gets the snitch~~

Ohio State wins but doesn't cover. Ohio State by 4.5.

---

```{r}
#| message: false
#| warning: false
#| fig-height: 6.5
#| fig-align: center
game_info |>
  filter(grepl("National Championship", notes)) |>
  inner_join(
    championship_sims
  ) |>
  add_game_outcomes() |>
  plot_playoff_sims(text = T)

```



#

wrapping up

---

The value of data science is simply that of science - it is the process by which we understand the world around us.

. . .

It allows us to discover [cause and effect]{.fg style="--col: #75BAFF"}; it allows us to [measure things we care about]{.fg style="--col: #75BAFF"}. It helps us understand the data we have and the data that we don’t.

---

To recap:

1) The data **you choose to collect, and not collect**, is part of the scientific process.

2) All of the technology in the world does not matter if you do not **understand your problem** and the **data and methodology that would help you solve it**.

3) Models are about more than just prediction; they enable us to **make sense of patterns in data** and **measure things we care about**.

4) The best work and the best predictions tend to come from **really trying to understand the thing you are predicting**.

#

one final thought

---

There is no easy button; there is no tool that you can buy and start solving all of your problems.

. . .

The value in (data) science usually doesn't come from algorithms, tools, platforms.

. . .

The value in (data) science is usually from the [creativity/dedication/passion]{.fg style="--col: #75BAFF"} of asking questions and caring about finding the answer.


---

I don't think data science is [electricity]{.fg style="--col: #C6B90B"} or data the [new oil]{.fg style="--col: #C6B90B"}. I think it's something much simpler.

. . .

(Data) science is like [farming]{.fg style="--col: #75BAFF"}.

. . .

It’s slow and difficult and takes a lot of patience.

. . .

But if you work at it and make an effort everyday, you will produce something valuable in the end.

#

thanks for listening

#

references

---

1) Mahmood, Syed S., et al. ["The Framingham Heart Study and the epidemiology of cardiovascular disease: a historical perspective."](https://pmc.ncbi.nlm.nih.gov/articles/PMC4159698/) The lancet 383.9921 (2014): 999-1008.

2) Dawber, Thomas R., Gilcin F. Meadors, and Felix E. Moore Jr. ["Epidemiological approaches to heart disease: the Framingham Study."](https://ajph.aphapublications.org/doi/pdf/10.2105/AJPH.41.3.279) American Journal of Public Health and the Nations Health 41.3 (1951): 279-286.

3) [Epidemiological Background and Design:
The Framingham Heart Study](https://www.framinghamheartstudy.org/fhs-about/history/epidemiological-background/)

4) Carter, Virgil, and Robert E. Machol. ["Operations research on football."](https://pubsonline.informs.org/doi/epdf/10.1287/opre.19.2.541) Operations Research 19.2 (1971): 541-544.
